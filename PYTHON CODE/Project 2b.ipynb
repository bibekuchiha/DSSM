{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "37213241",
   "metadata": {},
   "source": [
    "# Project 2b\n",
    "####  Use NLTK •Sample of text we are processing: •\n",
    "`This movie made it into one of my top 10 most awful movies. Horrible. There wasn't a continuous minute where there wasn't a fight with one monster or another. There was no chance for any character development, they were too busy running from one sword fight to another. I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them). •Write python code to clean the above text using the textual data cleaning methods discussed earlier and implement all the strategies as functions. •Using the functions created for cleaning generate summary of the above data by applying TF/IDF scoring.`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "d5b70cb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: contractions in c:\\programdata\\anaconda3\\lib\\site-packages (0.1.68)\n",
      "Requirement already satisfied: textsearch>=0.0.21 in c:\\programdata\\anaconda3\\lib\\site-packages (from contractions) (0.0.21)\n",
      "Requirement already satisfied: pyahocorasick in c:\\programdata\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (1.4.4)\n",
      "Requirement already satisfied: anyascii in c:\\programdata\\anaconda3\\lib\\site-packages (from textsearch>=0.0.21->contractions) (0.3.1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package maxent_ne_chunker to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Lenovo\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#importing Libraries\n",
    "!pip install contractions\n",
    "import contractions \n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize  # ['i', 'play', 'cricket']\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import string\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk import tokenize\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "nltk.download('maxent_ne_chunker')\n",
    "nltk.download(\"words\")\n",
    "# installing stopwords.\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "20c90d67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "3e10b63c",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"This movie made it into one of my top 10 most awful movies. Horrible. I don’t care if it makes 1 million, 10 M , or 100. There wasn't a continuous minute where there wasn't a fight with one monster or another. There was no chance for any character development, they were too busy running from one sword fight to another. I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them). If you disagree with me, you can send your thoughts to idonotcare@leavemealone.com\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "22c0c681",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie made it into one of my top 10 most awful movies. Horrible. I don’t care if it makes 1 million, 10 M , or 100. There wasn't a continuous minute where there wasn't a fight with one monster or another. There was no chance for any character development, they were too busy running from one sword fight to another. I had no emotional attachment ( except to the big bad machine ## that wanted to destroy them). If you disagree with me, you can send your thoughts to idonotcare@leavemealone.com\n"
     ]
    }
   ],
   "source": [
    "print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "09bf5a49",
   "metadata": {},
   "outputs": [],
   "source": [
    "#creating a class for text pre processing\n",
    "class Text_Wrap(object):  #nik = Text_Wrap(st)\n",
    "    def __init__ (self,st):\n",
    "        self.st= st\n",
    "        self.tokens = []\n",
    "        self.filtered= []\n",
    "\n",
    "    def reg_exp(self):\n",
    "        \"\"\" This function will perform pre-processing on the sample text by lowering the text, remove the numbers and special characters \n",
    "            \n",
    "            :return self.st = precprocessed text\n",
    "        \"\"\"\n",
    "\n",
    "        #self.st = self.st.lower()\n",
    "        self.st = re.sub('\\d+',' number',self.st)   #1 million -> number million                         #substituting digits with 'numbers'\n",
    "        self.st = re.sub('\\w[!-?A-~]*@\\w[\\w\\.]*\\w','email_id', self.st)     #for emails \n",
    "        self.st = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", self.st) #to substitute special characters with \"\"\n",
    "        self.st = re.sub('\\s{2,}', ' ', self.st) #remove extra spaces\n",
    "        return self.st\n",
    "\n",
    "\n",
    "    def expand_contractions(self):\n",
    "      \"\"\" This function will expand all the contactions i.e expanding the words with apostrophe\n",
    "      \n",
    "          :return st.text = st after expanding contactions\n",
    "\n",
    "      \"\"\"\n",
    "      expand_st = []\n",
    "      for i in self.st.split():\n",
    "          expand_st.append(contractions.fix(i)) \n",
    "      self.st = ' '.join(expand_st)\n",
    "\n",
    "      return self.st\n",
    "\n",
    "\n",
    "    def tokenization(self,type):\n",
    "      \"\"\" This function will tokenize the text either sentence or word \n",
    "\n",
    "          :return self.tokens = tokens\n",
    "\n",
    "      \"\"\"\n",
    "\n",
    "      if type == 'sentence_tokenize': \n",
    "            self.tokens = nltk.tokenize.sent_tokenize(self.st)\n",
    "      else:\n",
    "            self.tokens = nltk.tokenize.word_tokenize(self.st)           \n",
    "        \n",
    "      return self.tokens  \n",
    "\n",
    "\n",
    "    def stopword_removal (self, language = 'english'):\n",
    "\n",
    "      \"\"\" This function will remove the english language stopwords\n",
    "\n",
    "          :return self.filter : st after filtering.\n",
    "      \"\"\"\n",
    "      stop_words = set(stopwords.words(language))     #   ['switch', 'off','the',  'light']     ['the','a','an','this']                    \n",
    "      self.filter = [i for i in self.tokens if not i.lower() in stop_words] \n",
    "      \n",
    "      return self.filter #['switch', 'off', 'light']\n",
    "\n",
    "    def text_normalize(self,t_type):\n",
    "      \"\"\"\n",
    "      This function will either perform Stemming or Lemmatization\n",
    "\n",
    "      :return self.text\n",
    "\n",
    "      \"\"\"\n",
    "      if t_type == 'stem':   \n",
    "          ps = PorterStemmer()\n",
    "          self.text = [ps.stem(i) for i in self.filter]             \n",
    "      elif t_type == 'lemm':\n",
    "          lem = WordNetLemmatizer()\n",
    "          self.text = [lem.lemmatize(i) for i in self.filter]        \n",
    "      \n",
    "      return self.text    \n",
    "\n",
    "\n",
    "    def Result(self):\n",
    "      \"\"\"\n",
    "      This function will get the final result \n",
    "      \"\"\"\n",
    "      return self.text\n",
    "\n",
    "    def tfidf(self, corpus_texts):\n",
    "        \"\"\"\n",
    "        This method will apply the TFIDF to the list of sentences\n",
    "        in the arguments and return the vectors.\n",
    "\n",
    "        :return sentences_vectors: Numpy array of vectors\n",
    "        \"\"\"\n",
    "        wordfreq = {}\n",
    "        for sentence in corpus_texts:\n",
    "            # sent_processing = ProcessingText(sentence)\n",
    "            self.st = sentence\n",
    "            processed_sent = self.complete_processing()\n",
    "            for token in processed_sent:\n",
    "                if token not in wordfreq.keys():\n",
    "                    wordfreq[token] = 1\n",
    "                else:\n",
    "                    wordfreq[token] += 1\n",
    "        vocab = wordfreq\n",
    "        # tf and idf\n",
    "        termDict={}\n",
    "        docsTFMat = np.zeros((len(corpus_texts),len(vocab)))\n",
    "        docsIdfMat = np.zeros((len(vocab),len(corpus_texts)))\n",
    "\n",
    "        docTermDf = pd.DataFrame(docsTFMat ,columns=sorted(vocab.keys()))\n",
    "        docCount=0\n",
    "        for doc in corpus_texts:\n",
    "            self.st = doc\n",
    "            doc = self.complete_processing()\n",
    "            \n",
    "            for word in doc:\n",
    "                if (word in vocab.keys()):\n",
    "                    \n",
    "                    docTermDf[word][docCount] = docTermDf[word][docCount] +1\n",
    "\n",
    "            docCount = docCount +1\n",
    "\n",
    "\n",
    "        #Computed idf for each word in vocab\n",
    "        idfDict={}\n",
    "\n",
    "        for column in docTermDf.columns:\n",
    "            idfDict[column]= np.log((len(corpus_texts) +1 )/(1+ (docTermDf[column] != 0).sum()))+1\n",
    "\n",
    "        #compute tf.idf matrix\n",
    "        docsTfIdfMat = np.zeros((len(corpus_texts),len(vocab)))\n",
    "        docTfIdfDf = pd.DataFrame(docsTfIdfMat ,columns=sorted(vocab.keys()))\n",
    "\n",
    "\n",
    "\n",
    "        docCount = 0\n",
    "        for doc in corpus_texts:\n",
    "            for key in idfDict.keys():\n",
    "                docTfIdfDf[key][docCount] = docTermDf[key][docCount] * idfDict[key]\n",
    "            docCount = docCount +1\n",
    "        return docTfIdfDf\n",
    "\n",
    "\n",
    "\n",
    "    def complete_processing(self, tokens = 'word_tokenize', t_type='lemm'):\n",
    "      \"\"\"\n",
    "      This function will perform all the functions mentioned above and retuen the pre processed text.\n",
    "      \n",
    "      :return = Preprocessed text\n",
    "     \n",
    "      \"\"\"\n",
    "      self.reg_exp()\n",
    "      self.expand_contractions()\n",
    "      self.tokenization(tokens)\n",
    "      self.stopword_removal()\n",
    "      self.text_normalize(t_type)\n",
    "      return self.Result()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "0f0f8eb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_preprocessing = Text_Wrap(sentence)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "3130944b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.Text_Wrap at 0x239eafadc70>"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "c84f85bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie made it into one of my top number most awful movies Horrible I dont care if it makes number million number M or number There wasnt a continuous minute where there wasnt a fight with one monster or another There was no chance for any character development they were too busy running from one sword fight to another I had no emotional attachment except to the big bad machine that wanted to destroy them If you disagree with me you can send your thoughts to emailid\n"
     ]
    }
   ],
   "source": [
    "Text_output = (text_preprocessing.reg_exp())\n",
    "print(Text_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "726199f3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This movie made it into one of my top number most awful movies Horrible I do not care if it makes number million number M or number There was not a continuous minute where there was not a fight with one monster or another There was no chance for any character development they were too busy running from one sword fight to another I had no emotional attachment except to the big bad machine that wanted to destroy them If you disagree with me you can send your thoughts to emailid\n"
     ]
    }
   ],
   "source": [
    "#Removing Contractions\n",
    "C_output = text_preprocessing.expand_contractions()\n",
    "print(C_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "617f6406",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'movie', 'made', 'it', 'into', 'one', 'of', 'my', 'top', 'number', 'most', 'awful', 'movies', 'Horrible', 'I', 'do', 'not', 'care', 'if', 'it', 'makes', 'number', 'million', 'number', 'M', 'or', 'number', 'There', 'was', 'not', 'a', 'continuous', 'minute', 'where', 'there', 'was', 'not', 'a', 'fight', 'with', 'one', 'monster', 'or', 'another', 'There', 'was', 'no', 'chance', 'for', 'any', 'character', 'development', 'they', 'were', 'too', 'busy', 'running', 'from', 'one', 'sword', 'fight', 'to', 'another', 'I', 'had', 'no', 'emotional', 'attachment', 'except', 'to', 'the', 'big', 'bad', 'machine', 'that', 'wanted', 'to', 'destroy', 'them', 'If', 'you', 'disagree', 'with', 'me', 'you', 'can', 'send', 'your', 'thoughts', 'to', 'emailid']\n"
     ]
    }
   ],
   "source": [
    "# Tokenization\n",
    "T_output = text_preprocessing.tokenization('word_tokenize')\n",
    "print(T_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "91f5a9bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'made', 'one', 'top', 'number', 'awful', 'movies', 'Horrible', 'care', 'makes', 'number', 'million', 'number', 'number', 'continuous', 'minute', 'fight', 'one', 'monster', 'another', 'chance', 'character', 'development', 'busy', 'running', 'one', 'sword', 'fight', 'another', 'emotional', 'attachment', 'except', 'big', 'bad', 'machine', 'wanted', 'destroy', 'disagree', 'send', 'thoughts', 'emailid']\n"
     ]
    }
   ],
   "source": [
    "S_output = text_preprocessing.stopword_removal()\n",
    "print(S_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "812af509",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41\n"
     ]
    }
   ],
   "source": [
    "print(len(S_output))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "cd6b3246",
   "metadata": {},
   "outputs": [],
   "source": [
    "T_output = text_preprocessing.text_normalize('lemm')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96263848",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['movie', 'made', 'one', 'top', 'number', 'awful', 'movie', 'Horrible', 'care', 'make', 'number', 'million', 'number', 'number', 'continuous', 'minute', 'fight', 'one', 'monster', 'another', 'chance', 'character', 'development', 'busy', 'running', 'one', 'sword', 'fight', 'another', 'emotional', 'attachment', 'except', 'big', 'bad', 'machine', 'wanted', 'destroy', 'disagree', 'send', 'thought', 'emailid']\n"
     ]
    }
   ],
   "source": [
    "print(T_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "ed0b8e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "sent_1 = 'This is a bad movie.'\n",
    "sent_2 = 'Totally worth .'\n",
    "sent_3 = 'so bad movie'\n",
    "sent_4 = 'I had a good time watching movie.'\n",
    "sent_5 = 'This cinema  was amazing.'\n",
    "sent_6 = 'This was great.'\n",
    "\n",
    "corpus_text = [sent_1, sent_2, sent_3, sent_4, sent_5, sent_6]\n",
    "tfidf = (text_preprocessing.tfidf(corpus_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "9451d7ce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Totally   amazing       bad    cinema      good     great     movie  \\\n",
      "0  0.000000  0.000000  1.847298  0.000000  0.000000  0.000000  1.559616   \n",
      "1  2.252763  0.000000  0.000000  0.000000  0.000000  0.000000  0.000000   \n",
      "2  0.000000  0.000000  1.847298  0.000000  0.000000  0.000000  1.559616   \n",
      "3  0.000000  0.000000  0.000000  0.000000  2.252763  0.000000  1.559616   \n",
      "4  0.000000  2.252763  0.000000  2.252763  0.000000  0.000000  0.000000   \n",
      "5  0.000000  0.000000  0.000000  0.000000  0.000000  2.252763  0.000000   \n",
      "\n",
      "       time  watching     worth  \n",
      "0  0.000000  0.000000  0.000000  \n",
      "1  0.000000  0.000000  2.252763  \n",
      "2  0.000000  0.000000  0.000000  \n",
      "3  2.252763  2.252763  0.000000  \n",
      "4  0.000000  0.000000  0.000000  \n",
      "5  0.000000  0.000000  0.000000  \n"
     ]
    }
   ],
   "source": [
    "print(tfidf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a08371a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
